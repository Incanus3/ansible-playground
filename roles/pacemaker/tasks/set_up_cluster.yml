---
- set_fact:
    hosts_with_role:
      "{{ groups[cluster.role_group] }}"
    hosts_in_network_group:
      "{{ hostvars | dict2items
        | selectattr('value.network_group', 'eq', cluster.network_group)
        | map(attribute = 'key') | list }}"
- set_fact:
    hosts_in_cluster: "{{ hosts_with_role | intersect(hosts_in_network_group) }}"

- block:
    - debug:
        msg: |-
          setting up pacemaker for network group {{ cluster.network_group }} and role {{ cluster.role_group }}
          hosts with role: {{ hosts_with_role }}
          hosts in network group: {{ hosts_in_network_group }}
          hosts in cluster: {{ hosts_in_cluster }}
      run_once: true

    - name: install python expect library
      apt: name=python3-pexpect state=present

    - name: install pacemaker packages
      apt: name=pacemaker,pacemaker-cli-utils,fence-agents,pcs,resource-agents-paf state=present
    - name: allow connections to necessary ports
      ufw: rule=allow proto={{ item.proto }} port={{ item.port }} src={{ ansible_facts[private_net_iface] | iface_to_subnet_cidr }}
      with_items:
        - { proto: 'tcp', port: 2224  }
        - { proto: 'tcp', port: 3121  }
        - { proto: 'tcp', port: 21064 }
        - { proto: 'udp', port: 5405  }
    - name: enable pacemaker services
      service: name={{ item }} enabled=yes
      with_items: [corosync, pacemaker, pcsd]
    - name: start pcs daemon
      service: name=pcsd state=started

    - name: check if hacluster has set password
      shell: passwd --status hacluster | cut -f 2 -d ' '
      register: hacluster_password_status
      check_mode: no
      changed_when: false
    - name: set hacluster password if not set
      expect:
        command: passwd hacluster
        responses:
          (?i)password: "{{ pacemaker_hacluster_password }}"
      when: hacluster_password_status.stdout == 'L'

    - name: check if pcs command is available
      shell: which pcs
      register: which_pcs
      check_mode: no
      failed_when: false
      changed_when: false

    - block:
        - name: check for initial (after-install) cluster
          command: pcs status
          register: pcs_status_initial
          check_mode: no
          failed_when: false
          changed_when: false

        - name: destroy initial cluster
          command: pcs cluster destroy
          when: "'Cluster name: debian' in pcs_status_initial.stdout_lines"

        - block:
            - name: check for existing cluster
              command: pcs status
              register: pcs_status
              check_mode: no
              failed_when: false
              changed_when: false

            - name: set cluster_already_existed variable
              set_fact:
                cluster_already_existed: "{{ 'Cluster name: ' + cluster.name in pcs_status.stdout_lines }}"

            - block:
                - name: authenticate all nodes in cluster with each other
                  command: >-
                    pcs cluster auth {{ hosts_in_cluster | join(' ') }}
                    -u "{{ pacemaker_hacluster_username }}"
                    -p "{{ pacemaker_hacluster_password }}"
                  register: result
                  changed_when: result.stdout_lines | reject('search', 'Already authorized') | list

                # TODO: support alternative host addresses to fall back on other network
                - name: initialize cluster
                  command: pcs cluster setup --name "{{ cluster.name }}" {{ hosts_in_cluster | join(' ') }}

                - name: start cluster
                  command: pcs cluster start --all

                - name: set cluster recheck interval (default is too long)
                  command: pcs property set cluster-recheck-interval={{ pacemaker_cluster_recheck_interval }}
                  retries: 3
                  delay: 1
                  register: result
                  until: ansible_check_mode or result.rc == 0

                - name: set default resource migration threshold
                  command: pcs resource defaults migration-threshold={{ pacemaker_default_migration_threshold }}

                - name: set default resource stickiness
                  command: pcs resource defaults resource-stickiness={{ pacemaker_default_resource_stickiness }}

                - name: set helper variables for fencing
                  set_fact:
                    cluster_with_fencing: "{{ cluster | add_default_fencing(hosts_in_cluster) }}"
                    additional_constraints: []

                - name: disable fencing
                  command: pcs property set stonith-enabled=false
                  when: not cluster_with_fencing.fencing

              when: not cluster_already_existed

          when: inventory_hostname == hosts_in_cluster[0]

        - name: get existing fences
          shell: pcs stonith show
          register: pcs_fences
          check_mode: no
          failed_when: false
          changed_when: false

        - name: get existing resources
          shell: pcs resource show --full | grep -P '^ (Master|Resource):'
          register: pcs_resources
          check_mode: no
          failed_when: false
          changed_when: false

        - name: get existing constraints
          shell: pcs constraint --full | grep -Po '(?<=\(id:).*(?=\))'
          register: pcs_constraints
          check_mode: no
          failed_when: false
          changed_when: false

        - name: set variables for existing entities
          set_fact:
            existing_cluster_fences:      "{{ pcs_fences.stdout_lines    | map('split') | map('first')  | list }}"
            existing_cluster_resources:   "{{ pcs_resources.stdout_lines | map('split') | map('nth', 2) | list }}"
            existing_cluster_constraints: "{{ pcs_constraints.stdout_lines }}"

        - block:
            - name: create shadow cluster configuration
              command: pcs cluster cib {{ cib_file_path }}
              when: inventory_hostname == hosts_in_cluster[0]
              changed_when: false

            - name: save starting cluster configuration
              copy: src="{{ cib_file_path }}" dest="{{ cib_start_path }}" remote_src=true
              when: not ansible_check_mode and inventory_hostname == hosts_in_cluster[0]
              changed_when: false

            - include_tasks: set_up_fence.yml
              when: inventory_hostname == hosts_in_cluster[0]
              loop: "{{ cluster_with_fencing.fencing }}"
              loop_control:
                loop_var: fence

            - include_tasks: set_up_resource.yml
              when: resource.name not in existing_cluster_resources
              loop: "{{ cluster.resources | default([]) }}"
              loop_control:
                loop_var: resource

            - include_tasks: set_up_constraint.yml
              when: inventory_hostname == hosts_in_cluster[0]
              loop: "{{ cluster.constraints | default([]) + additional_constraints }}"
              loop_control:
                loop_var: constraint

            - name: check if there are changes to apply
              shell: "crm_diff --original {{ cib_start_path }} --new {{ cib_file_path }} > /dev/null"
              when: inventory_hostname == hosts_in_cluster[0]
              register: crm_diff
              failed_when: crm_diff.rc > 1
              changed_when: false

            - name: commit shadow cluster configuration
              command: pcs cluster cib-push {{ cib_file_path }}
              when: not ansible_check_mode and inventory_hostname == hosts_in_cluster[0] and crm_diff.rc == 1

          always:
            - name: remove shadow config
              file: state=absent path="{{ cib_file_path  }}"
              when: inventory_hostname == hosts_in_cluster[0]
              changed_when: false

            - name: remove starting config
              file: state=absent path="{{ cib_start_path }}"
              when: inventory_hostname == hosts_in_cluster[0]
              changed_when: false

          vars:
            cib_file_path:  /tmp/ansible.xml
            cib_start_path: /tmp/ansible-start.xml

      when: which_pcs.rc == 0

    # for some reason, corosync and pacemaker services get disabled during cluster setup
    - name: enable pacemaker services
      service: name={{ item }} enabled=yes state=started
      with_items: [corosync, pacemaker, pcsd]

  when: hosts_in_cluster | length > 1 and inventory_hostname in hosts_in_cluster
